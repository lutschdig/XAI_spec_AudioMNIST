{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"01_Preprocessing_Training_Evaluation.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm","authorship_tag":"ABX9TyPtmTSlWx2vYlmAnrZZ4JAX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"4SVmFI4AF51l"},"source":["**Recommendations:**\n","1. **Use Colab Pro+ with high-RAM runtime** (data is stored in RAM to decrease execution time): If the session crashes after using all available RAM (might happen if you train the models cross-validated with less RAM then recommended) --> rerun all cells above. It will skip steps which were already finished automatically.\n","2. **Use GPU runtime for model training**"]},{"cell_type":"markdown","metadata":{"id":"kmX3n02GtI6n"},"source":["# 1. Basic code\n","**Always needs to be executed even if some steps are skipped!**\n","\n","The results of most steps are stored when the step is finished. Each step starts with reading the result of the previous step. This allows skipping steps, if the previous steps were already executed sometime else.\n","\n","The main steps are:\n","1. Preprocessing (create spectrograms and splits)\n","2. Train the models\n","3. Evaluate the models\n","4. Run XAI methods on the models (see other script?)\n","5. Create Plots of waveform/spectrograms"]},{"cell_type":"markdown","metadata":{"id":"kX191KucObES"},"source":["## Mount Google Drive"]},{"cell_type":"code","metadata":{"id":"D4_bqiiJS7e0"},"source":["from google.colab import drive #mount Google Drive\n","drive.mount('/gdrive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QDWMUU-jJSTB"},"source":["# current provided GPU\n","!nvidia-smi"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KIjvyr89MDjs"},"source":["## Imports"]},{"cell_type":"code","metadata":{"id":"OZD_D4-yMDJz"},"source":["# for importing from py scripts in the root directory (including subfolders)\n","import sys\n","\n","# for function get_duration\n","import math\n","import time\n","\n","# for function write_log\n","import logging\n","\n","# for preprocessing Samek\n","import numpy as np\n","import glob\n","import os\n","import sys\n","import scipy.io.wavfile as wavf\n","import scipy.signal\n","import h5py\n","import json\n","import librosa\n","import multiprocessing\n","#import argparse # dont need this one\n","\n","# for preprocessing adjustments\n","from urllib.request import urlopen\n","from io import BytesIO\n","from zipfile import ZipFile\n","# import shutil\n","\n","# for reading hdf5 files\n","import h5py\n","\n","# for model training\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Flatten, Conv2D\n","from tensorflow.keras.losses import sparse_categorical_crossentropy\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow import keras\n","from tensorflow.keras import backend as k\n","import pickle\n","import tensorflow as tf\n","\n","\n","# for model evaluation\n","from tensorflow.keras.models import Model, load_model\n","import pandas as pd\n","from sklearn.metrics import accuracy_score, balanced_accuracy_score, cohen_kappa_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve\n","from sklearn.preprocessing import minmax_scale\n","import matplotlib.pyplot as plt\n","\n","# for xai methods\n","\n","# for generating plots\n","from PIL import Image,ImageOps\n","from keras.preprocessing.image import img_to_array,array_to_img\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uZ1ZKKh-OiVZ"},"source":["## Set paths and import py scripts"]},{"cell_type":"code","metadata":{"id":"3nC1IPtaTCbF"},"source":["# set root dir: the path to the github repo folder \"XAI_spec_TSC\" in your google drive\n","root_dir = '/gdrive/My Drive/XAI_spec_AudioMNIST/'\n","\n","paths = {\n","    'root': root_dir,\n","    'dataset': os.path.join(root_dir,'AudioMNIST-master'),\n","    'data': os.path.join(root_dir,'AudioMNIST-master/data'),\n","    'meta': os.path.join(root_dir,'AudioMNIST-master/data/audioMNIST_meta.txt'),\n","    'spectrograms': os.path.join(root_dir,'spectrograms'),\n","    'splits': os.path.join(root_dir,'splits'),\n","    'results': os.path.join(root_dir, 'results'),\n","    'models': os.path.join(root_dir, 'results/models'),\n","    'history': os.path.join(root_dir, 'results/history'),\n","    'evaluation': os.path.join(root_dir, 'results/evaluation'),\n","    #'xai': os.path.join(root_dir, 'results/xai'),\n","    'plots': os.path.join(root_dir, 'results/plots'),\n","    # 'plots_waveform': os.path.join(root_dir, 'results/plots/waveform'),\n","    # 'plots_spectrograms': os.path.join(root_dir, 'results/plots/spectrograms'),\n","    'predictions': os.path.join(root_dir, 'results/predictions'),\n","    'plots_loss': os.path.join(root_dir, 'results/plots/loss')\n","}\n","\n","# labels and number of splits\n","labels = ['gender','digit']\n","splits = 5\n","\n","# append the directory to the python path using sys in order to make the seperate py files importable\n","sys.path.append(root_dir)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ltTEpMgqLIs4"},"source":["## Utils\n","Contains useful functions, which are used frequently during the code / in severeal sections"]},{"cell_type":"code","metadata":{"id":"Oeafwh-QLTZ3"},"source":["########## 1. function to write log about the events\n","def write_log(message):\n","    logging.basicConfig(level = logging.INFO, filename=os.path.join(root_dir,'events.log'), filemode='a', format='%(asctime)s - %(message)s')\n","    logging.info(message)\n","    print(message) \n","\n","\n","########## 2. function to create a directory\n","def create_directory(directory_path):\n","    if os.path.isdir(directory_path) == False:\n","        os.mkdir(directory_path)\n","        write_log('Created folder: '+directory_path)\n","    #else:\n","        #writeLog('Folder already exists: '+directory_path)\n","\n","########## 3. function to calculate time difference between two timepoints from package 'time' and returns the duration in format HH:MM:SS as String\n","def get_duration(start_time,end_time):\n","    duration = round(end_time-start_time)\n","    if duration < 0:\n","        duration*=-1\n","    h=math.floor(duration/3600)\n","    r=duration%3600\n","    m=math.floor(r/60)\n","    r=r%60\n","    s=round(r)\n","    return(str(h).zfill(2)+':'+str(m).zfill(2)+':'+str(s).zfill(2))\n","\n","########### 4. function to read spectrograms/labels and return np.arrays ready for training/evaluation/xai methods\n","def read_spectrograms_hdf5(label,split_index,split_type,resize_factor=1,reshape=False,img_width=227, img_height=227, img_num_channels=1):\n","  write_log('Started reading '+str(split_type)+' data ...')\n","  start_time = time.time()\n","  if label == 'gender':\n","    label_index = 1\n","  else:\n","    label_index = 0\n","  # read txt with current split paths\n","  path_to_split_paths = os.path.join(paths['splits'],'AlexNet_'+str(label)+'_'+str(split_index)+'_'+str(split_type)+'.txt')\n","  text_file = open(path_to_split_paths, 'r')\n","  split_paths = text_file.read().split('\\n')\n","  text_file.close()\n","  # if there are empty lines at the end of the txt file there will be an empty list element for each empty line\n","  # removing empty lines/list elements\n","  while split_paths[len(split_paths)-1] == '':\n","    split_paths.pop(len(split_paths)-1)\n","  # read hdf5 files of the current split and split_type and store it as np.array (spectrograms as x and labels as y)\n","  index = 0\n","  x = np.zeros(((len(split_paths),227,227))) # create target array for spectrograms\n","  y = np.zeros(len(split_paths)) # create target array for labels\n","  for cur_path in split_paths: # iterate the files\n","    #read current file\n","    f = h5py.File(cur_path, 'r')\n","    x_cur = f['data'][...]\n","    y_cur = f['label'][...]\n","    f.close() \n","    #extract relevant data of current file\n","    x_cur = x_cur[0][0]\n","    y_cur = y_cur[0][label_index]    \n","    #append current data to x and y\n","    x[index] = x_cur\n","    y[index] = y_cur\n","    # increase index by 1\n","    index +=1\n","  x = x/resize_factor\n","  if reshape:\n","    x = x.reshape((len(x), img_width, img_height, img_num_channels))\n","  write_log('Finished reading '+str(split_type)+' data in '+get_duration(start_time,time.time()))\n","  return x,y\n","\n","#create directories for the results\n","for path in paths:\n","  if 'result' in paths[path]:\n","    create_directory(paths[path])\n","\n","def get_split_paths(path_to_split_paths):\n","  text_file = open(path_to_split_paths, 'r')\n","  split_paths = text_file.read().split('\\n')\n","  text_file.close()\n","  # if there are empty lines at the end of the txt file there will be an empty list element for each empty line\n","  # removing empty lines/list elements\n","  while split_paths[len(split_paths)-1] == '':\n","    split_paths.pop(len(split_paths)-1)\n","  return split_paths"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_YkAT_3xOwZe"},"source":["# 2. Preprocessing\n","Creates spectrograms in HDF5 format and splits as txt files."]},{"cell_type":"markdown","metadata":{"id":"i_HO1Y5iP-tq"},"source":["## Preprocessing from Samek et al."]},{"cell_type":"code","metadata":{"id":"wq3_2llJTEBV"},"source":["# the following code is the preprocessing from https://github.com/soerenab/AudioMNIST/blob/master/preprocess_data.py\n","# I did not write this whole code snippet\n","# adjustments were made, see next cell\n","def preprocess_data(src, dst, src_meta, n_processes=15):\n","\n","    \"\"\"\n","    Calls for distibuted preprocessing of the data.\n","    Parameters:\n","    -----------\n","        src: string\n","            Path to data directory.\n","        dst: string\n","            Path to directory where preprocessed data shall be stored.\n","        stc_meta: string\n","            Path to meta_information file.\n","        n_processes: int\n","            number of simultaneous processes to use for data preprocessing.\n","    \"\"\"\n","\n","    folders = []\n","\n","    for folder in os.listdir(src):\n","        # only process folders\n","        if not os.path.isdir(os.path.join(src, folder)):\n","            continue\n","        folders.append(folder)\n","\n","    pool=multiprocessing.Pool(processes=n_processes)\n","    _=pool.map(_preprocess_data, [(os.path.join(src, folder), \n","                                          os.path.join(dst, folder), \n","                                          src_meta) for folder in sorted(folders)])\n","\n","def _preprocess_data(src_dst_meta):\n","\n","    \"\"\"\n","    Preprocessing for all data files in given directory.\n","    Preprocessing includes:\n","        AlexNet: resampling to 8000 Hz, \n","                 embedding in zero vector, \n","                 transformation to amplitute spectrogram representation in dB.\n","        \n","        AudioNet: resampling to 8000 Hz, \n","                  embedding in zero vector, \n","                  normalization at 95th percentile.\n","    Preprocessed data will be stored in hdf5 files with one datum per file.\n","    In terms of I/O, this is not very efficient but it allows to easily change\n","    training, validation, and test sets without re-preprocessing or redundant \n","    storage of preprocessed files.\n","    Parameters:\n","    -----------\n","        src_dst_meta: tuple of 3 strings\n","            Tuple (path to data directory, path to destination directory, path\n","            to meta file)\n","    \"\"\"\n","\n","\n","    src, dst, src_meta = src_dst_meta\n","\n","    print(\"processing {}\".format(src))\n","\n","    metaData = json.load(open(src_meta))\n","\n","    # create folder for hdf5 files\n","    if not os.path.exists(dst):\n","        os.makedirs(dst)\n","    # loop over recordings\n","    for filepath in sorted(glob.glob(os.path.join(src, \"*.wav\"))):\n","\n","        # infer sample info from name\n","        dig, vp, rep = filepath.rstrip(\".wav\").split(\"/\")[-1].split(\"_\")\n","        # read data\n","        fs, data = wavf.read(filepath)\n","        # resample\n","        data = librosa.core.resample(y=data.astype(np.float32), orig_sr=fs, target_sr=8000, res_type=\"scipy\")\n","        # zero padding\n","        if len(data) > 8000:\n","            raise ValueError(\"data length cannot exceed padding length.\")\n","        elif len(data) < 8000:\n","            embedded_data = np.zeros(8000)\n","            offset = np.random.randint(low = 0, high = 8000 - len(data))\n","            embedded_data[offset:offset+len(data)] = data\n","        elif len(data) == 8000:\n","            # nothing to do here\n","            embedded_data = data\n","            pass\n","\n","        ##### AlexNet #####\n","\n","        # stft, with seleced parameters, spectrogram will have shape (228,230)\n","        f, t, Zxx = scipy.signal.stft(embedded_data, 8000, nperseg = 455, noverlap = 420, window='hann')\n","        # get amplitude\n","        Zxx = np.abs(Zxx[0:227, 2:-1])\n","        Zxx = np.atleast_3d(Zxx).transpose(2,0,1)\n","        # convert to decibel\n","        Zxx = librosa.amplitude_to_db(Zxx, ref = np.max)\n","        # save as hdf5 file\n","        with h5py.File(os.path.join(dst, \"AlexNet_{}_{}_{}.hdf5\".format(dig, vp, rep)), \"w\") as f:\n","            tmp_X = np.zeros([1, 1, 227, 227])\n","\n","            tmp_X[0, 0] = Zxx\n","            f['data'] = tmp_X\n","            f['label'] = np.array([[int(dig), 0 if metaData[vp][\"gender\"] == \"male\" else 1]])\n","\n","        # ##### AudioNet #####\n","        \n","        # embedded_data /= (np.percentile(embedded_data, 95) + 0.001)\n","        \n","        # with h5py.File(os.path.join(dst, \"AudioNet_{}_{}_{}.hdf5\".format(dig, vp, rep)), \"w\") as f:\n","        #     tmp_X = np.zeros([1, 1, 1, 8000])\n","\n","        #     tmp_X[0, 0, 0] = embedded_data\n","        #     f['data'] = tmp_X\n","        #     f['label'] = np.array([[int(dig), 0 if metaData[vp][\"gender\"] == \"male\" else 1]])\n","\n","    return\n","\n","def create_splits(src, dst):\n","\n","    \"\"\"\n","    Creation of text files specifying which files training, validation and test\n","    set consist of for each cross-validation split. \n","    Parameters:\n","    -----------\n","        src: string\n","            Path to directory containing the directories for each subject that\n","            hold the preprocessed data in hdf5 format.\n","        dst: string\n","            Destination where to store the text files specifying training, \n","            validation and test splits.\n","    \"\"\"\n","\n","    #print(\"creating splits\")\n","    splits={\"digit\":{   \"train\":[   set([28, 56,  7, 19, 35,  1,  6, 16, 23, 34, 46, 53, 36, 57,  9, 24, 37,  2, \\\n","                                          8, 17, 29, 39, 48, 54, 43, 58, 14, 25, 38,  3, 10, 20, 30, 40, 49, 55]),\n","\n","                                    set([36, 57,  9, 24, 37,  2,  8, 17, 29, 39, 48, 54, 43, 58, 14, 25, 38,  3, \\\n","                                         10, 20, 30, 40, 49, 55, 12, 47, 59, 15, 27, 41,  4, 11, 21, 31, 44, 50]),\n","\n","                                    set([43, 58, 14, 25, 38,  3, 10, 20, 30, 40, 49, 55, 12, 47, 59, 15, 27, 41, \\\n","                                          4, 11, 21, 31, 44, 50, 26, 52, 60, 18, 32, 42,  5, 13, 22, 33, 45, 51]),\n","\n","                                    set([12, 47, 59, 15, 27, 41,  4, 11, 21, 31, 44, 50, 26, 52, 60, 18, 32, 42, \\\n","                                          5, 13, 22, 33, 45, 51, 28, 56,  7, 19, 35,  1,  6, 16, 23, 34, 46, 53]),\n","\n","                                    set([26, 52, 60, 18, 32, 42,  5, 13, 22, 33, 45, 51, 28, 56,  7, 19, 35,  1, \\\n","                                          6, 16, 23, 34, 46, 53, 36, 57,  9, 24, 37,  2,  8, 17, 29, 39, 48, 54])],\n","\n","                        \"validate\":[set([12, 47, 59, 15, 27, 41,  4, 11, 21, 31, 44, 50]),\n","                                    set([26, 52, 60, 18, 32, 42,  5, 13, 22, 33, 45, 51]),\n","                                    set([28, 56,  7, 19, 35,  1,  6, 16, 23, 34, 46, 53]),\n","                                    set([36, 57,  9, 24, 37,  2,  8, 17, 29, 39, 48, 54]),\n","                                    set([43, 58, 14, 25, 38,  3, 10, 20, 30, 40, 49, 55])],\n","\n","                        \"test\":[    set([26, 52, 60, 18, 32, 42,  5, 13, 22, 33, 45, 51]),\n","                                    set([28, 56,  7, 19, 35,  1,  6, 16, 23, 34, 46, 53]),\n","                                    set([36, 57,  9, 24, 37,  2,  8, 17, 29, 39, 48, 54]),\n","                                    set([43, 58, 14, 25, 38,  3, 10, 20, 30, 40, 49, 55]),\n","                                    set([12, 47, 59, 15, 27, 41,  4, 11, 21, 31, 44, 50])]},\n","\n","            \"gender\":{  \"train\":[   set([36, 47, 56, 26, 12, 57, 2, 44, 50, 25, 37, 45]),\n","                                    set([26, 12, 57, 43, 28, 52, 25, 37, 45, 48, 53, 41]),\n","                                    set([43, 28, 52, 58, 59, 60, 48, 53, 41, 7, 23, 38]),\n","                                    set([58, 59, 60, 36, 47, 56, 7, 23, 38, 2, 44, 50])],\n","\n","                        \"validate\":[set([43, 28, 52, 48, 53, 41]),\n","                                    set([58, 59, 60, 7, 23, 38]),\n","                                    set([36, 47, 56, 2, 44, 50]),\n","                                    set([26, 12, 57, 25, 37, 45])],\n","\n","                        \"test\":[    set([58, 59, 60, 7, 23, 38]),\n","                                    set([36, 47, 56, 2, 44, 50]),\n","                                    set([26, 12, 57, 25, 37, 45]),\n","                                    set([43, 28, 52, 48, 53, 41])]}}\n","\n","    for split in range(5):\n","        for modus in [\"train\", \"validate\", \"test\"]:\n","            for task in [\"digit\", \"gender\"]:\n","                if task == \"gender\" and split > 3:\n","                    continue\n","                with open(os.path.join(dst, \"AlexNet_{}_{}_{}.txt\".format(task, split, modus)), mode = \"w\") as txt_file:\n","                    for vp in splits[task][modus][split]:\n","                        for filepath in glob.glob(os.path.join(src, \"{:02d}\".format(vp), \"AlexNet*.hdf5\")):\n","                            txt_file.write(filepath+\"\\n\")\n","\n","                # with open(os.path.join(dst, \"AudioNet_{}_{}_{}.txt\".format(task, split, modus)), mode = \"w\") as txt_file:\n","                #     for vp in splits[task][modus][split]:\n","                #         for filepath in glob.glob(os.path.join(src, \"{:02d}\".format(vp), \"AudioNet*.hdf5\")):\n","                #             txt_file.write(filepath+\"\\n\")\n","\n","\n","######################################################## adjustment starting here #######################################################################################################\n","\n","# if __name__ == \"__main__\":\n","\n","#     parser = argparse.ArgumentParser()\n","#     parser.add_argument('--source', '-src', default=os.path.join(os.getcwd(), \"data\"), help=\"Path to folder containing each participant's data directory.\")\n","#     parser.add_argument('--destination', '-dst', default=os.path.join(os.getcwd(), \"preprocessed_data\"), help=\"Destination where preprocessed data shall be stored.\")\n","#     parser.add_argument('--meta', '-m', default=os.path.join(os.getcwd(), \"data\", \"audioMNIST_meta.txt\"), help=\"Path to meta_information json file.\")\n","\n","#     args = parser.parse_args()\n","\n","#     # preprocessing\n","#     preprocess_data(src=args.source, dst=args.destination, src_meta=args.meta)\n","#     # create training, validation and test sets\n","#     create_splits(src=args.destination, dst=args.destination)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"81FnP9XAPzHW"},"source":["## Adjustments and extensions for the preprocessing"]},{"cell_type":"code","metadata":{"id":"LHRIqMdWPw-j"},"source":["# download dataset AudioMNIST\n","if os.path.isdir(paths['dataset']) == False:\n","  try:\n","    start_time = time.time()\n","    url = 'https://github.com/soerenab/AudioMNIST/archive/refs/heads/master.zip'\n","    write_log('Downloading AudioMNIST dataset ...')\n","    http_response = urlopen(url)\n","    zipfile = ZipFile(BytesIO(http_response.read()))\n","    write_log('Unzipping AudioMNIST dataset ...')\n","    zipfile.extractall(path=paths['root'])\n","    write_log('Download sucessful, dataset is ready now! (download time: '+get_duration(start_time,time.time())+')')\n","  except:\n","    write_log('AudioMNIST download failed!')\n","else:\n","  write_log('Data already exists, no download necessary!')\n","\n","# # paths for preprocessing\n","# data_dir = os.path.join(dataset_dir,'data')\n","# metafile_path = os.path.join(dataset_dir,'data','audioMNIST_meta.txt')\n","# spectrogram_dir = os.path.join(root_dir,'spectrograms')\n","# split_dir = os.path.join(root_dir,'splits')\n","\n","# start preprocessing\n","\n","# create spectrograms: hdf5 file with spectrogram data and labels (digit and gender) for each wav-file\n","if os.path.isdir(paths['spectrograms']) == False:\n","  start_time = time.time()\n","  write_log('Creating spectrograms ...')\n","  preprocess_data(src=paths['data'], dst=paths['spectrograms'], src_meta=paths['meta'])\n","  write_log('Spectrograms created in '+get_duration(start_time,time.time()))\n","else:\n","  write_log('Spectrograms already exist, no preprocessing necessary!')\n","\n","# creat splits\n","if os.path.isdir(paths['splits']) == False:\n","  create_directory(paths['splits'])\n","  start_time = time.time()\n","  # create training, validation and test sets\n","  write_log('Creating splits ...')\n","  create_splits(src=paths['spectrograms'], dst=paths['splits'])\n","  write_log('Splits created in '+get_duration(start_time,time.time()))\n","else:\n","  write_log('Splits already exist, no preprocessing necessary!')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G1RTEf0RtfxN"},"source":["# 3. Train models"]},{"cell_type":"markdown","metadata":{"id":"0bp7KNg3tDnl"},"source":["## General network settings"]},{"cell_type":"code","metadata":{"id":"1KA8S4gQeGya"},"source":["# Model configuration\n","batch_size = 32 # spectrograms per step. Tested 32,64,128,1000\n","img_width, img_height, img_num_channels = 227, 227, 1 # image size and channels (3=RGB, 1=Grayscale)\n","input_shape = (img_width, img_height, img_num_channels) # input shape for the first layer\n","loss_function = sparse_categorical_crossentropy # loss function\n","no_epochs = 100 # rounds of training\n","optimizer = tf.optimizers.SGD(learning_rate=0.001)  # optimizer Adam(amsgrad=True)\n","verbosity = 1\n","resize_factor = 255 #255"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kgCFU-ndCRv9"},"source":["## Generate model architecture"]},{"cell_type":"code","metadata":{"id":"hfkQ2YdbCVCx"},"source":["def get_model_architecture(input_shape,no_classes,activation):\n","  model = keras.models.Sequential([\n","    keras.layers.Conv2D(filters=96, kernel_size=(11,11), strides=(4,4), activation='relu', input_shape=input_shape),\n","    keras.layers.BatchNormalization(),\n","    keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),\n","    keras.layers.Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), activation='relu', padding=\"same\"),\n","    keras.layers.BatchNormalization(),\n","    keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),\n","    keras.layers.Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n","    keras.layers.BatchNormalization(),\n","    keras.layers.Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n","    keras.layers.BatchNormalization(),\n","    keras.layers.Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n","    keras.layers.BatchNormalization(),\n","    keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),\n","    keras.layers.Flatten(),\n","    keras.layers.Dense(1024, activation='relu'), #4096\n","    keras.layers.Dropout(0.5),\n","    keras.layers.Dense(1024, activation='relu'), #4096\n","    keras.layers.Dropout(0.5),\n","    keras.layers.Dense(no_classes, activation=activation)\n","  ])\n","  print(model.summary())\n","  return model   "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dIkjlte_g0_L"},"source":["## start model trainings"]},{"cell_type":"code","metadata":{"id":"iw-ovuPscq-e"},"source":["write_log('Started training of the models!')\n","# iterate labels and splits\n","for label in labels:\n","  for split in range(0,splits):\n","# for label in ['gender']: #replace, its only for testing\n","#   for split in range(1,2): # replace, its only for testing\n","    if label == \"gender\" and split > 3: # there are only 4 splits for the label gender, whereas there are 5 for label digit\n","      continue\n","    write_log('Current label: '+str(label)+' and current split: '+str(split))\n","    #check if the model already exists\n","    model_saving_path = os.path.join(paths['models'],'AlexNet_'+label+'_'+str(split)+'.h5')\n","    history_saving_path = os.path.join(paths['history'],'AlexNet_'+label+'_'+str(split)+'.pkl')\n","    if os.path.isfile(model_saving_path) == False:\n","      # call functions to read data for train and validation\n","      x_train, y_train = read_spectrograms_hdf5(label,split,'train',resize_factor,reshape=True)\n","      x_val, y_val = read_spectrograms_hdf5(label,split,'validate',resize_factor,reshape=True)\n","      # set number of classes\n","      if label == 'gender':\n","        no_classes = 2\n","        activation = 'sigmoid'\n","      else: \n","        no_classes = 10\n","        activation = 'softmax'\n","      # get model structure\n","      model = get_model_architecture(input_shape,no_classes,activation)\n","      # Compile the model\n","      model.compile(loss=loss_function, #loss='binary_crossentropy'\n","              optimizer=optimizer,\n","              metrics=['accuracy'])\n","      # train current model\n","      write_log('Training the model ...')\n","      start_time = time.time()\n","      history = model.fit(x_train, y_train,\n","            batch_size=batch_size,\n","            epochs=no_epochs,\n","            verbose=verbosity,\n","            validation_data = (x_val, y_val))\n","      write_log('Training finished in '+get_duration(start_time,time.time()))\n","      # save current model and history\n","      write_log('Saving the model and history ...')\n","      pickle.dump(history.history, open(history_saving_path, 'wb'))\n","      model.save(model_saving_path)\n","      write_log('Model and history saved!')\n","      # the data is stacked in the RAM for each training run, in order to clear the RAM and prevent RAM exceed the following lines release some RAM\n","      k.clear_session()\n","      try:\n","        del x_train\n","        del y_train\n","        del x_val\n","        del y_val\n","      except:\n","        pass\n","    else:\n","      write_log('Model already exists')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G4dDaLiNO03i"},"source":["# 4. Evaluate models"]},{"cell_type":"markdown","metadata":{"id":"yh4jJQZHYYEP"},"source":["## Evaluate all models"]},{"cell_type":"code","metadata":{"id":"e4wN1ownO1Ms"},"source":["# create list of trained models\n","write_log('Started creating evaluations ...')\n","model_names = os.listdir(paths['models'])\n","# get meta data to get gender later\n","metaData = json.load(open(paths['meta']))\n","# iterate models\n","for model_name in model_names:\n","  # get label and split from modelname\n","  net,label,split_index = model_name.rstrip(\".h5\").split(\"/\")[-1].split(\"_\")\n","  # set saving paths for the results\n","  evaluation_path = os.path.join(paths['evaluation'],'evaluation_'+label+'_'+str(split_index)+'.csv')\n","  confusion_matrix_path = os.path.join(paths['evaluation'],'confusionMatrix_'+label+'_'+str(split_index)+'.csv')\n","  predictions_saving_path = os.path.join(paths['predictions'],'predictions_'+label+'_'+str(split_index)+'.csv')\n","  # check if model was already evaluated\n","  if os.path.isfile(evaluation_path) and os.path.isfile(confusion_matrix_path) and os.path.isfile(predictions_saving_path):\n","    write_log('Model for '+label+' '+str(split_index)+' already evaluated')\n","  else:\n","    write_log('Current model: '+label+' '+str(split_index))\n","    # load current model\n","    cur_model = load_model(os.path.join(paths['models'],model_name))\n","    # read according test data\n","    x_test, y_test = read_spectrograms_hdf5(label,split_index,'test',resize_factor=255,reshape=True)\n","    # Run the Class Predictions and get the probabilities\n","    start_time = time.time()\n","    pred_probas = cur_model.predict(x_test, verbose=1)\n","    predictions = np.argmax(pred_probas,axis=1)\n","    if label == 'gender':\n","      predictions = np.where(predictions==0,'male','female')\n","    # create dataframe with predictions for XAI-inspections\n","    if os.path.isfile(predictions_saving_path) == False:\n","      predictions_df = pd.DataFrame(data=np.zeros((len(predictions),5),dtype=np.float), columns=['digit','participant','repetition','actual','predicted'])\n","      path_to_split_paths = os.path.join(paths['splits'],'AlexNet_'+label+'_'+str(split_index)+'_test.txt')\n","      split_paths = get_split_paths(path_to_split_paths)\n","      digits, participants, repetitions, actual = [],[],[],[]\n","      for split_path in split_paths:\n","        net, dig, vp, rep = split_path[:len(split_path)-5].split('/')[-1].split('_') # infer sample info from name\n","        digits.append(int(dig))\n","        participants.append(int(vp))\n","        repetitions.append(int(rep))\n","        if label == 'gender':\n","          actual.append(metaData[vp]['gender'])\n","      if label == 'digit':\n","        actual = digits\n","      predictions_df['digit'] = digits\n","      predictions_df['participant'] = participants\n","      predictions_df['repetition'] = repetitions\n","      predictions_df['actual'] = actual\n","      predictions_df['predicted'] = predictions\n","      predictions_df.to_csv(predictions_saving_path, index=False)\n","      write_log('Created prediction csv!')  \n","    # calculate evaluation metrics\n","    if os.path.isfile(evaluation_path) == False:\n","      evaluation = pd.DataFrame(data=np.zeros((1,7),dtype=np.float), columns=['label','split','accuracy','balanced accuracy','precision','recall','kappa'])\n","      evaluation['label'] = label\n","      evaluation['split'] = split_index\n","      evaluation['accuracy'] = accuracy_score(y_test, predictions)\n","      evaluation['balanced accuracy'] = balanced_accuracy_score(y_test, predictions)\n","      evaluation['precision'] = precision_score(y_test, predictions, average='weighted')\n","      evaluation['recall'] = recall_score(y_test, predictions, average='weighted')\n","      evaluation['kappa'] = cohen_kappa_score(y_test, predictions)\n","      #roc_AUC = roc_auc_score(y_test, predictions, average = 'macro', multi_class='ovo') # not working for multiclass?!\n","      evaluation.to_csv(evaluation_path, index=False)\n","      write_log('Created evaluation csv!') \n","    # calculate confusion matrix\n","    if os.path.isfile(confusion_matrix_path) == False:\n","      confusion_matrix = pd.crosstab(y_test, predictions, rownames=['Actual'], colnames=['Predicted'])\n","      confusion_matrix.to_csv(confusion_matrix_path, index= False)\n","      write_log('Created confusion matrix csv!') \n","    write_log('Evaluation of model '+label+' '+str(split_index)+' finished in '+str(get_duration(start_time,time.time())))\n","    # the data is stacked in the RAM for each model evaluation, in order to clear the RAM and prevent RAM exceed the following lines release some RAM\n","    k.clear_session()\n","    try:\n","      del x_test\n","      del y_test\n","    except:\n","      pass\n","write_log('Finished creating evaluations!')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"X0N3zAsSYTjC"},"source":["## Calculate mean results"]},{"cell_type":"code","metadata":{"id":"ToTWSHLs_lsa"},"source":["write_log('Calculating mean results ...')\n","start_time = time.time()\n","# create variables to store the results\n","gender_evaluation_data = pd.DataFrame(data=np.zeros((0,7),dtype=np.float), columns=['label','split','accuracy','balanced accuracy','precision','recall','kappa'])\n","gender_confusion_matrix_data = pd.DataFrame(data=np.zeros((0,0),dtype=np.float))\n","digit_evaluation_data = pd.DataFrame(data=np.zeros((0,7),dtype=np.float), columns=['label','split','accuracy','balanced accuracy','precision','recall','kappa'])\n","digit_confusion_matrix_data = pd.DataFrame(data=np.zeros((0,0),dtype=np.float))\n","# define counters\n","gender_confusion_matrix_counter,digit_confusion_matrix_counter = 0,0\n","# iterate all available result files\n","for cur_evaluation_name in os.listdir(paths['evaluation']):\n","  # exclude possibly existing mean files\n","  if 'mean' in cur_evaluation_name:\n","    continue\n","  # get evaluation type, label and split from saved result csv\n","  evaluation_type,label,split_index = cur_evaluation_name.rstrip(\".csv\").split(\"/\")[-1].split(\"_\")\n","  # read results of the models\n","  cur_data = pd.read_csv(os.path.join(paths['evaluation'],cur_evaluation_name))\n","  # add current results to the correct data collection\n","  if evaluation_type == 'evaluation':\n","    if label == 'gender':\n","      gender_evaluation_data = pd.concat((gender_evaluation_data,cur_data),axis=0,sort=False)\n","    else:\n","      digit_evaluation_data = pd.concat((digit_evaluation_data,cur_data),axis=0,sort=False)\n","  else:\n","    if label == 'gender':\n","      gender_confusion_matrix_data = gender_confusion_matrix_data.add(cur_data, fill_value=0)\n","      gender_confusion_matrix_counter += 1\n","    else:\n","      digit_confusion_matrix_data = digit_confusion_matrix_data.add(cur_data, fill_value=0)\n","      digit_confusion_matrix_counter += 1\n","# calculate mean values\n","if gender_evaluation_data.size > 0:\n","  gender_evaluation_data.loc['mean'] = gender_evaluation_data.mean()\n","  gender_evaluation_data.at['mean', 'label'] ='gender'\n","  gender_evaluation_data.at['mean', 'split'] ='mean'\n","if digit_evaluation_data.size > 0:\n","  digit_evaluation_data.loc['mean'] = digit_evaluation_data.mean()\n","  digit_evaluation_data.at['mean', 'label'] ='digit'\n","  digit_evaluation_data.at['mean', 'split'] ='mean'\n","gender_confusion_matrix_mean = gender_confusion_matrix_data/gender_confusion_matrix_counter\n","digit_confusion_matrix_mean = digit_confusion_matrix_data/digit_confusion_matrix_counter\n","gender_confusion_matrix_mean_percentage = gender_confusion_matrix_mean/1500\n","digit_confusion_matrix_mean_percentage = digit_confusion_matrix_mean/600\n","# save mean results to csv\n","gender_evaluation_data.to_csv(os.path.join(paths['evaluation'],'evaluation_gender_mean.csv'),index = False)\n","digit_evaluation_data.to_csv(os.path.join(paths['evaluation'],'evaluation_digit_mean.csv'),index = False)\n","gender_confusion_matrix_mean.to_csv(os.path.join(paths['evaluation'],'confusionMatrix_gender_mean.csv'),index = False)\n","digit_confusion_matrix_mean.to_csv(os.path.join(paths['evaluation'],'confusionMatrix_digit_mean.csv'),index = False)\n","gender_confusion_matrix_mean_percentage.to_csv(os.path.join(paths['evaluation'],'confusionMatrix_gender_mean_percentage.csv'),index = False)\n","digit_confusion_matrix_mean_percentage.to_csv(os.path.join(paths['evaluation'],'confusionMatrix_digit_mean_percentage.csv'),index = False)\n","write_log('Mean results calculated in '+get_duration(start_time,time.time()))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1uagze8ci0KH"},"source":["## Model loss plots"]},{"cell_type":"code","metadata":{"id":"PTKPBDtjHpu1"},"source":["write_log('Started creating model loss plots ...')\n","for history in os.listdir(paths['history']):\n","  # get label and slit from history name\n","  _, label, split = history.rstrip(\".pkl\").split(\"_\")\n","  # define saving path\n","  plot_saving_path = os.path.join(paths['plots_loss'],'Loss_'+label+'_'+str(split)+'.png')\n","  # skip if plot already exists\n","  if os.path.isfile(plot_saving_path):\n","    continue\n","  # read history file\n","  history = pickle.load(open(os.path.join(paths['history'],history),'rb'))\n","  #create plot\n","  plt.plot(history['loss'])\n","  plt.plot(history['val_loss'])\n","  plt.title('Model loss: '+label+' '+str(split))\n","  plt.ylabel('Loss')\n","  plt.xlabel('Epoch')\n","  plt.legend(['train', 'validate'], loc='center right')\n","  # save and show plot\n","  plt.savefig(plot_saving_path)\n","  plt.show()\n","  plt.close()\n","write_log('Finished creating model loss plots')"],"execution_count":null,"outputs":[]}]}