{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"01_Preprocessing_Training_Evaluation.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyOLvvA6a4jzbs0d29XiLyGK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"kmX3n02GtI6n"},"source":["# 1. Basic code\n","**Always needs to be executed even if some steps are skipped!**\n","\n","The results of most steps are stored when the step is finished. Each step starts with reading the result of the previous step. This allows skipping steps, if the previous steps were already executed sometime else.\n","\n","The main steps are:\n","1. Preprocessing (create spectrograms and splits)\n","2. Train the models\n","3. Evaluate the models\n","4. Run XAI methods on the models (see other script?)\n","5. Create Plots of waveform/spectrograms"]},{"cell_type":"markdown","metadata":{"id":"kX191KucObES"},"source":["## Mount Google Drive"]},{"cell_type":"code","metadata":{"id":"D4_bqiiJS7e0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630662324886,"user_tz":-120,"elapsed":19227,"user":{"displayName":"Philipp RÃ¶ssler","photoUrl":"","userId":"08136704935029710025"}},"outputId":"8f1dd5cf-d596-4a90-ba03-23e938d11566"},"source":["from google.colab import drive #mount Google Drive\n","drive.mount('/gdrive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /gdrive\n"]}]},{"cell_type":"markdown","metadata":{"id":"KIjvyr89MDjs"},"source":["## Imports"]},{"cell_type":"code","metadata":{"id":"OZD_D4-yMDJz"},"source":["# for importing from py scripts in the root directory (including subfolders)\n","import sys\n","\n","# for function get_duration\n","import math\n","import time\n","\n","# for function write_log\n","import logging\n","\n","# for preprocessing Samek\n","import numpy as np\n","import glob\n","import os\n","import sys\n","import scipy.io.wavfile as wavf\n","import scipy.signal\n","import h5py\n","import json\n","import librosa\n","import multiprocessing\n","#import argparse # dont need this one\n","\n","# for preprocessing adjustments\n","from urllib.request import urlopen\n","from io import BytesIO\n","from zipfile import ZipFile\n","# import shutil\n","\n","# for reading hdf5 files\n","import h5py\n","\n","# for model training\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Flatten, Conv2D\n","from tensorflow.keras.losses import sparse_categorical_crossentropy\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow import keras\n","import pickle\n","import tensorflow as tf\n","\n","\n","# for model evaluation\n","from tensorflow.keras.models import Model, load_model\n","import pandas as pd\n","from sklearn.metrics import accuracy_score, balanced_accuracy_score, cohen_kappa_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve\n","from sklearn.preprocessing import minmax_scale\n","import matplotlib.pyplot as plt\n","\n","# for xai methods\n","\n","\n","\n","\n","#########\n","# import numpy as np\n","# import pandas as pd\n","# import matplotlib.pyplot as plt\n","\n","# import warnings\n","\n","# from tensorflow.keras.applications import VGG16 #Xception #ResNet50, VGG16\n","# from tensorflow.keras.models import Model, load_model\n","# from tensorflow.keras import layers\n","# from tensorflow.keras import optimizers\n","# from tensorflow.keras.regularizers import l2\n","# from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","# from tensorflow.keras.callbacks import ModelCheckpoint\n","\n","# import tensorflow as tf\n","\n","# from sklearn.metrics import accuracy_score, cohen_kappa_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve\n","# from sklearn.model_selection import KFold, train_test_split\n","# import pickle\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uZ1ZKKh-OiVZ"},"source":["## Set paths and import py scripts"]},{"cell_type":"code","metadata":{"id":"3nC1IPtaTCbF"},"source":["# set root dir: the path to the github repo folder \"XAI_spec_TSC\" in your google drive\n","root_dir = '/gdrive/My Drive/XAI_spec_AudioMNIST/'\n","\n","paths = {\n","    'root': root_dir,\n","    'dataset': os.path.join(root_dir,'AudioMNIST-master'),\n","    'data': os.path.join(root_dir,'AudioMNIST-master/data'),\n","    'meta': os.path.join(root_dir,'AudioMNIST-master/data/audioMNIST_meta.txt'),\n","    'spectrograms': os.path.join(root_dir,'spectrograms'),\n","    'splits': os.path.join(root_dir,'splits'),\n","    'results': os.path.join(root_dir, 'results'),\n","    'models': os.path.join(root_dir, 'results/models'),\n","    'history': os.path.join(root_dir, 'results/history'),\n","    'evaluation': os.path.join(root_dir, 'results/evaluation'),\n","    'xai': os.path.join(root_dir, 'results/xai'),\n","    'plots': os.path.join(root_dir, 'results/plots'),\n","    'plots_waveform': os.path.join(root_dir, 'results/plots/waveform'),\n","    'plots_spectrograms': os.path.join(root_dir, 'results/plots/spectrograms')\n","}\n","\n","# labels and number of splits\n","labels = ['gender','digit']\n","splits = 5\n","\n","# append the directory to the python path using sys in order to make the seperate py files importable\n","sys.path.append(root_dir)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ltTEpMgqLIs4"},"source":["## Utils\n","Contains useful functions, which are used frequently during the code / in severeal sections"]},{"cell_type":"code","metadata":{"id":"Oeafwh-QLTZ3"},"source":["########## 1. function to write log about the events\n","def write_log(message):\n","    logging.basicConfig(level = logging.INFO, filename=os.path.join(root_dir,'events.log'), filemode='a', format='%(asctime)s - %(message)s')\n","    logging.info(message)\n","    print(message) \n","\n","\n","########## 2. function to create a directory\n","def create_directory(directory_path):\n","    if os.path.isdir(directory_path) == False:\n","        os.mkdir(directory_path)\n","        write_log('Created folder: '+directory_path)\n","    #else:\n","        #writeLog('Folder already exists: '+directory_path)\n","\n","########## 3. function to calculate time difference between two timepoints from package 'time' and returns the duration in format HH:MM:SS as String\n","def get_duration(start_time,end_time):\n","    duration = round(end_time-start_time)\n","    if duration < 0:\n","        duration*=-1\n","    h=math.floor(duration/3600)\n","    r=duration%3600\n","    m=math.floor(r/60)\n","    r=r%60\n","    s=round(r)\n","    return(str(h).zfill(2)+':'+str(m).zfill(2)+':'+str(s).zfill(2))\n","\n","########### 4. function to read spectrograms/labels and return np.arrays ready for training/evaluation/xai methods\n","def read_data(label,split_index,split_type):\n","  write_log('Started reading '+str(split_type)+' data ...')\n","  start_time = time.time()\n","  if label == 'gender':\n","    label_index = 1\n","  else:\n","    label_index = 0\n","  # read txt with current split paths\n","  path_to_split_paths = os.path.join(paths['splits'],'AlexNet_'+str(label)+'_'+str(split_index)+'_'+str(split_type)+'.txt')\n","  text_file = open(path_to_split_paths, 'r')\n","  split_paths = text_file.read().split('\\n')\n","  text_file.close()\n","  # if there are empty lines at the end of the txt file there will be an empty list element for each empty line\n","  # removing empty lines/list elements\n","  while split_paths[len(split_paths)-1] == '':\n","    split_paths.pop(len(split_paths)-1)\n","  # read hdf5 files of the current split and split_type and store it as np.array (spectrograms as x and labels as y)\n","  index = 0\n","  x = np.zeros(((len(split_paths),227,227))) # create target array for spectrograms\n","  y = np.zeros(len(split_paths)) # create target array for labels\n","  for cur_path in split_paths: # iterate the files\n","    #read current file\n","    f = h5py.File(cur_path, 'r')\n","    x_cur = f['data'][...]\n","    y_cur = f['label'][...]\n","    f.close() \n","    #extract relevant data of current file\n","    x_cur = x_cur[0][0]\n","    y_cur = y_cur[0][label_index]    \n","    #append current data to x and y\n","    x[index] = x_cur\n","    y[index] = y_cur\n","    # increase index by 1\n","    index +=1\n","  write_log('Finished reading '+str(split_type)+' data in '+get_duration(start_time,time.time()))\n","  return x,y\n","\n","#create directories for the results\n","for path in paths:\n","  if 'result' in paths[path]:\n","    create_directory(paths[path])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_YkAT_3xOwZe"},"source":["# 2. Preprocessing\n","Creates spectrograms in HDF5 format and splits as txt files."]},{"cell_type":"markdown","metadata":{"id":"i_HO1Y5iP-tq"},"source":["## Preprocessing from Samek et al."]},{"cell_type":"code","metadata":{"id":"wq3_2llJTEBV"},"source":["# the following code is the preprocessing from https://github.com/soerenab/AudioMNIST/blob/master/preprocess_data.py\n","# I did not write this whole code snippet\n","# adjustments were made, see next cell\n","def preprocess_data(src, dst, src_meta, n_processes=15):\n","\n","    \"\"\"\n","    Calls for distibuted preprocessing of the data.\n","    Parameters:\n","    -----------\n","        src: string\n","            Path to data directory.\n","        dst: string\n","            Path to directory where preprocessed data shall be stored.\n","        stc_meta: string\n","            Path to meta_information file.\n","        n_processes: int\n","            number of simultaneous processes to use for data preprocessing.\n","    \"\"\"\n","\n","    folders = []\n","\n","    for folder in os.listdir(src):\n","        # only process folders\n","        if not os.path.isdir(os.path.join(src, folder)):\n","            continue\n","        folders.append(folder)\n","\n","    pool=multiprocessing.Pool(processes=n_processes)\n","    _=pool.map(_preprocess_data, [(os.path.join(src, folder), \n","                                          os.path.join(dst, folder), \n","                                          src_meta) for folder in sorted(folders)])\n","\n","def _preprocess_data(src_dst_meta):\n","\n","    \"\"\"\n","    Preprocessing for all data files in given directory.\n","    Preprocessing includes:\n","        AlexNet: resampling to 8000 Hz, \n","                 embedding in zero vector, \n","                 transformation to amplitute spectrogram representation in dB.\n","        \n","        AudioNet: resampling to 8000 Hz, \n","                  embedding in zero vector, \n","                  normalization at 95th percentile.\n","    Preprocessed data will be stored in hdf5 files with one datum per file.\n","    In terms of I/O, this is not very efficient but it allows to easily change\n","    training, validation, and test sets without re-preprocessing or redundant \n","    storage of preprocessed files.\n","    Parameters:\n","    -----------\n","        src_dst_meta: tuple of 3 strings\n","            Tuple (path to data directory, path to destination directory, path\n","            to meta file)\n","    \"\"\"\n","\n","\n","    src, dst, src_meta = src_dst_meta\n","\n","    print(\"processing {}\".format(src))\n","\n","    metaData = json.load(open(src_meta))\n","\n","    # create folder for hdf5 files\n","    if not os.path.exists(dst):\n","        os.makedirs(dst)\n","    # loop over recordings\n","    for filepath in sorted(glob.glob(os.path.join(src, \"*.wav\"))):\n","\n","        # infer sample info from name\n","        dig, vp, rep = filepath.rstrip(\".wav\").split(\"/\")[-1].split(\"_\")\n","        # read data\n","        fs, data = wavf.read(filepath)\n","        # resample\n","        data = librosa.core.resample(y=data.astype(np.float32), orig_sr=fs, target_sr=8000, res_type=\"scipy\")\n","        # zero padding\n","        if len(data) > 8000:\n","            raise ValueError(\"data length cannot exceed padding length.\")\n","        elif len(data) < 8000:\n","            embedded_data = np.zeros(8000)\n","            offset = np.random.randint(low = 0, high = 8000 - len(data))\n","            embedded_data[offset:offset+len(data)] = data\n","        elif len(data) == 8000:\n","            # nothing to do here\n","            embedded_data = data\n","            pass\n","\n","        ##### AlexNet #####\n","\n","        # stft, with seleced parameters, spectrogram will have shape (228,230)\n","        f, t, Zxx = scipy.signal.stft(embedded_data, 8000, nperseg = 455, noverlap = 420, window='hann')\n","        # get amplitude\n","        Zxx = np.abs(Zxx[0:227, 2:-1])\n","        Zxx = np.atleast_3d(Zxx).transpose(2,0,1)\n","        # convert to decibel\n","        Zxx = librosa.amplitude_to_db(Zxx, ref = np.max)\n","        # save as hdf5 file\n","        with h5py.File(os.path.join(dst, \"AlexNet_{}_{}_{}.hdf5\".format(dig, vp, rep)), \"w\") as f:\n","            tmp_X = np.zeros([1, 1, 227, 227])\n","\n","            tmp_X[0, 0] = Zxx\n","            f['data'] = tmp_X\n","            f['label'] = np.array([[int(dig), 0 if metaData[vp][\"gender\"] == \"male\" else 1]])\n","\n","        ##### AudioNet #####\n","        \n","        embedded_data /= (np.percentile(embedded_data, 95) + 0.001)\n","        \n","        with h5py.File(os.path.join(dst, \"AudioNet_{}_{}_{}.hdf5\".format(dig, vp, rep)), \"w\") as f:\n","            tmp_X = np.zeros([1, 1, 1, 8000])\n","\n","            tmp_X[0, 0, 0] = embedded_data\n","            f['data'] = tmp_X\n","            f['label'] = np.array([[int(dig), 0 if metaData[vp][\"gender\"] == \"male\" else 1]])\n","\n","    return\n","\n","def create_splits(src, dst):\n","\n","    \"\"\"\n","    Creation of text files specifying which files training, validation and test\n","    set consist of for each cross-validation split. \n","    Parameters:\n","    -----------\n","        src: string\n","            Path to directory containing the directories for each subject that\n","            hold the preprocessed data in hdf5 format.\n","        dst: string\n","            Destination where to store the text files specifying training, \n","            validation and test splits.\n","    \"\"\"\n","\n","    #print(\"creating splits\")\n","    splits={\"digit\":{   \"train\":[   set([28, 56,  7, 19, 35,  1,  6, 16, 23, 34, 46, 53, 36, 57,  9, 24, 37,  2, \\\n","                                          8, 17, 29, 39, 48, 54, 43, 58, 14, 25, 38,  3, 10, 20, 30, 40, 49, 55]),\n","\n","                                    set([36, 57,  9, 24, 37,  2,  8, 17, 29, 39, 48, 54, 43, 58, 14, 25, 38,  3, \\\n","                                         10, 20, 30, 40, 49, 55, 12, 47, 59, 15, 27, 41,  4, 11, 21, 31, 44, 50]),\n","\n","                                    set([43, 58, 14, 25, 38,  3, 10, 20, 30, 40, 49, 55, 12, 47, 59, 15, 27, 41, \\\n","                                          4, 11, 21, 31, 44, 50, 26, 52, 60, 18, 32, 42,  5, 13, 22, 33, 45, 51]),\n","\n","                                    set([12, 47, 59, 15, 27, 41,  4, 11, 21, 31, 44, 50, 26, 52, 60, 18, 32, 42, \\\n","                                          5, 13, 22, 33, 45, 51, 28, 56,  7, 19, 35,  1,  6, 16, 23, 34, 46, 53]),\n","\n","                                    set([26, 52, 60, 18, 32, 42,  5, 13, 22, 33, 45, 51, 28, 56,  7, 19, 35,  1, \\\n","                                          6, 16, 23, 34, 46, 53, 36, 57,  9, 24, 37,  2,  8, 17, 29, 39, 48, 54])],\n","\n","                        \"validate\":[set([12, 47, 59, 15, 27, 41,  4, 11, 21, 31, 44, 50]),\n","                                    set([26, 52, 60, 18, 32, 42,  5, 13, 22, 33, 45, 51]),\n","                                    set([28, 56,  7, 19, 35,  1,  6, 16, 23, 34, 46, 53]),\n","                                    set([36, 57,  9, 24, 37,  2,  8, 17, 29, 39, 48, 54]),\n","                                    set([43, 58, 14, 25, 38,  3, 10, 20, 30, 40, 49, 55])],\n","\n","                        \"test\":[    set([26, 52, 60, 18, 32, 42,  5, 13, 22, 33, 45, 51]),\n","                                    set([28, 56,  7, 19, 35,  1,  6, 16, 23, 34, 46, 53]),\n","                                    set([36, 57,  9, 24, 37,  2,  8, 17, 29, 39, 48, 54]),\n","                                    set([43, 58, 14, 25, 38,  3, 10, 20, 30, 40, 49, 55]),\n","                                    set([12, 47, 59, 15, 27, 41,  4, 11, 21, 31, 44, 50])]},\n","\n","            \"gender\":{  \"train\":[   set([36, 47, 56, 26, 12, 57, 2, 44, 50, 25, 37, 45]),\n","                                    set([26, 12, 57, 43, 28, 52, 25, 37, 45, 48, 53, 41]),\n","                                    set([43, 28, 52, 58, 59, 60, 48, 53, 41, 7, 23, 38]),\n","                                    set([58, 59, 60, 36, 47, 56, 7, 23, 38, 2, 44, 50])],\n","\n","                        \"validate\":[set([43, 28, 52, 48, 53, 41]),\n","                                    set([58, 59, 60, 7, 23, 38]),\n","                                    set([36, 47, 56, 2, 44, 50]),\n","                                    set([26, 12, 57, 25, 37, 45])],\n","\n","                        \"test\":[    set([58, 59, 60, 7, 23, 38]),\n","                                    set([36, 47, 56, 2, 44, 50]),\n","                                    set([26, 12, 57, 25, 37, 45]),\n","                                    set([43, 28, 52, 48, 53, 41])]}}\n","\n","    for split in range(5):\n","        for modus in [\"train\", \"validate\", \"test\"]:\n","            for task in [\"digit\", \"gender\"]:\n","                if task == \"gender\" and split > 3:\n","                    continue\n","                with open(os.path.join(dst, \"AlexNet_{}_{}_{}.txt\".format(task, split, modus)), mode = \"w\") as txt_file:\n","                    for vp in splits[task][modus][split]:\n","                        for filepath in glob.glob(os.path.join(src, \"{:02d}\".format(vp), \"AlexNet*.hdf5\")):\n","                            txt_file.write(filepath+\"\\n\")\n","\n","                with open(os.path.join(dst, \"AudioNet_{}_{}_{}.txt\".format(task, split, modus)), mode = \"w\") as txt_file:\n","                    for vp in splits[task][modus][split]:\n","                        for filepath in glob.glob(os.path.join(src, \"{:02d}\".format(vp), \"AudioNet*.hdf5\")):\n","                            txt_file.write(filepath+\"\\n\")\n","\n","\n","######################################################## adjustment starting here #######################################################################################################\n","\n","# if __name__ == \"__main__\":\n","\n","#     parser = argparse.ArgumentParser()\n","#     parser.add_argument('--source', '-src', default=os.path.join(os.getcwd(), \"data\"), help=\"Path to folder containing each participant's data directory.\")\n","#     parser.add_argument('--destination', '-dst', default=os.path.join(os.getcwd(), \"preprocessed_data\"), help=\"Destination where preprocessed data shall be stored.\")\n","#     parser.add_argument('--meta', '-m', default=os.path.join(os.getcwd(), \"data\", \"audioMNIST_meta.txt\"), help=\"Path to meta_information json file.\")\n","\n","#     args = parser.parse_args()\n","\n","#     # preprocessing\n","#     preprocess_data(src=args.source, dst=args.destination, src_meta=args.meta)\n","#     # create training, validation and test sets\n","#     create_splits(src=args.destination, dst=args.destination)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"81FnP9XAPzHW"},"source":["## Adjustments and extensions for the preprocessing"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LHRIqMdWPw-j","executionInfo":{"status":"ok","timestamp":1630662377872,"user_tz":-120,"elapsed":501,"user":{"displayName":"Philipp RÃ¶ssler","photoUrl":"","userId":"08136704935029710025"}},"outputId":"e3555d57-35f4-4200-fbd9-83e85816cfc3"},"source":["# download dataset AudioMNIST\n","if os.path.isdir(paths['dataset']) == False:\n","  try:\n","    start_time = time.time()\n","    url = 'https://github.com/soerenab/AudioMNIST/archive/refs/heads/master.zip'\n","    write_log('Downloading AudioMNIST dataset ...')\n","    http_response = urlopen(url)\n","    zipfile = ZipFile(BytesIO(http_response.read()))\n","    write_log('Unzipping AudioMNIST dataset ...')\n","    zipfile.extractall(path=paths['root'])\n","    write_log('Download sucessful, dataset is ready now! (download time: '+get_duration(start_time,time.time()))\n","  except:\n","    write_log('AudioMNIST download failed!')\n","else:\n","  write_log('Data already exists, no download necessary!')\n","\n","# # paths for preprocessing\n","# data_dir = os.path.join(dataset_dir,'data')\n","# metafile_path = os.path.join(dataset_dir,'data','audioMNIST_meta.txt')\n","# spectrogram_dir = os.path.join(root_dir,'spectrograms')\n","# split_dir = os.path.join(root_dir,'splits')\n","\n","# start preprocessing\n","\n","# create spectrograms: hdf5 file with spectrogram data and labels (digit and gender) for each wav-file\n","if os.path.isdir(paths['spectrograms']) == False:\n","  start_time = time.time()\n","  write_log('Creating spectrograms ...')\n","  preprocess_data(src=paths['data'], dst=paths['spectrograms'], src_meta=paths['meta'])\n","  write_log('Spectrograms created in '+get_duration(start_time,time.time()))\n","else:\n","  write_log('Spectrograms already exist, no preprocessing necessary!')\n","\n","# creat splits\n","if os.path.isdir(paths['splits']) == False:\n","  create_directory(paths['splits'])\n","  start_time = time.time()\n","  # create training, validation and test sets\n","  write_log('Creating splits ...')\n","  create_splits(src=paths['spectrograms'], dst=paths['splits'])\n","  write_log('Splits created in '+get_duration(start_time,time.time()))\n","else:\n","  write_log('Splits already exist, no preprocessing necessary!')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Data already exists, no download necessary!\n","Spectrograms already exist, no preprocessing necessary!\n","Splits already exist, no preprocessing necessary!\n"]}]},{"cell_type":"markdown","metadata":{"id":"G1RTEf0RtfxN"},"source":["# 3. Train models"]},{"cell_type":"markdown","metadata":{"id":"0bp7KNg3tDnl"},"source":["## General network settings"]},{"cell_type":"code","metadata":{"id":"1KA8S4gQeGya","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630662381302,"user_tz":-120,"elapsed":188,"user":{"displayName":"Philipp RÃ¶ssler","photoUrl":"","userId":"08136704935029710025"}},"outputId":"e168530d-2867-4f98-bfb5-988243855c8d"},"source":["# Model configuration\n","batch_size = 32 # spectrograms per step\n","img_width, img_height, img_num_channels = 227, 227, 1 # image size and channels (3=RGB, 1=Grayscale)\n","input_shape = (img_width, img_height, img_num_channels) # input shape for the first layer\n","loss_function = sparse_categorical_crossentropy # loss function\n","no_epochs = 100 # rounds of training\n","optimizer = tf.optimizers.SGD(lr=0.001)  # optimizer Adam(amsgrad=True)\n","verbosity = 1\n","resize_factor = 256 #256"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"]}]},{"cell_type":"markdown","metadata":{"id":"kgCFU-ndCRv9"},"source":["## Generate model architecture"]},{"cell_type":"code","metadata":{"id":"hfkQ2YdbCVCx"},"source":["def get_model_architecture(input_shape,no_classes,activation):\n","  model = keras.models.Sequential([\n","    keras.layers.Conv2D(filters=96, kernel_size=(11,11), strides=(4,4), activation='relu', input_shape=input_shape),\n","    keras.layers.BatchNormalization(),\n","    keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),\n","    keras.layers.Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), activation='relu', padding=\"same\"),\n","    keras.layers.BatchNormalization(),\n","    keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),\n","    keras.layers.Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n","    keras.layers.BatchNormalization(),\n","    keras.layers.Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n","    keras.layers.BatchNormalization(),\n","    keras.layers.Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n","    keras.layers.BatchNormalization(),\n","    keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),\n","    keras.layers.Flatten(),\n","    keras.layers.Dense(1024, activation='relu'), #4096\n","    keras.layers.Dropout(0.5),\n","    keras.layers.Dense(1024, activation='relu'), #4096\n","    keras.layers.Dropout(0.5),\n","    keras.layers.Dense(no_classes, activation=activation)\n","  ])\n","  print(model.summary())\n","  return model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dIkjlte_g0_L"},"source":["## start model trainings"]},{"cell_type":"code","metadata":{"id":"iw-ovuPscq-e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630662385892,"user_tz":-120,"elapsed":183,"user":{"displayName":"Philipp RÃ¶ssler","photoUrl":"","userId":"08136704935029710025"}},"outputId":"06111f83-d798-4e96-e1d6-22512854adc9"},"source":["write_log('Started training of the models!')\n","# iterate labels and splits\n","for label in labels:\n","  for split in range(0,splits):\n","# for label in ['gender']: #replace, its only for testing\n","#   for split in range(1,2): # replace, its only for testing\n","    if label == \"gender\" and split > 3: # there are only 4 splits for the label gender, whereas there are 5 for label digit\n","      continue\n","    write_log('Current label: '+str(label)+' and current split: '+str(split))\n","    #check if the model already exists\n","    model_saving_path = os.path.join(paths['models'],'AlexNet_'+label+'_'+str(split)+'.h5')\n","    history_saving_path = os.path.join(paths['history'],'AlexNet_'+label+'_'+str(split)+'.pkl')\n","    if os.path.isfile(model_saving_path) == False:\n","      # call functions to read data for train and validation\n","      x_train, y_train = read_data(label,split,'train')\n","      x_val, y_val = read_data(label,split,'validate')\n","      # resize x\n","      x_train = x_train/resize_factor\n","      x_val = x_val/resize_factor\n","      # Reshape data, must be (no_spectrograms, 227,227,1) instead of (no_spectrograms, 227,227)\n","      x_train = x_train.reshape((len(x_train), img_width, img_height, img_num_channels))\n","      x_val  = x_val.reshape((len(x_val), img_width, img_height, img_num_channels))\n","      # set number of classes\n","      if label == 'gender':\n","        no_classes = 2\n","        activation = 'sigmoid'\n","      else: \n","        no_classes = 10\n","        activation = 'softmax'\n","      # get model structure\n","      model = get_model_architecture(input_shape,no_classes,activation)\n","      # Compile the model\n","      model.compile(loss=loss_function, #loss='binary_crossentropy'\n","              optimizer=optimizer,\n","              metrics=['accuracy'])\n","      # train current model\n","      write_log('Training the model ...')\n","      start_time = time.time()\n","      history = model.fit(x_train, y_train,\n","            batch_size=batch_size,\n","            epochs=no_epochs,\n","            verbose=verbosity,\n","            validation_data = (x_val, y_val))\n","      write_log('Training finished in '+get_duration(start_time,time.time()))\n","      # save current model and history\n","      write_log('Saving the model and history ...')\n","      pickle.dump(history.history, open(history_saving_path, 'wb'))\n","      model.save(model_saving_path)\n","      write_log('Model and history saved!')\n","    else:\n","      write_log('Model already exists')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Started training of the models!\n","Current label: gender and current split: 0\n","Model already exists\n","Current label: gender and current split: 1\n","Model already exists\n","Current label: gender and current split: 2\n","Model already exists\n","Current label: gender and current split: 3\n","Model already exists\n","Current label: digit and current split: 0\n","Model already exists\n","Current label: digit and current split: 1\n","Model already exists\n","Current label: digit and current split: 2\n","Model already exists\n","Current label: digit and current split: 3\n","Model already exists\n","Current label: digit and current split: 4\n","Model already exists\n"]}]},{"cell_type":"markdown","metadata":{"id":"G4dDaLiNO03i"},"source":["# 4. Evaluate models"]},{"cell_type":"markdown","metadata":{"id":"yh4jJQZHYYEP"},"source":["## Evaluate all models"]},{"cell_type":"code","metadata":{"id":"e4wN1ownO1Ms","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630662393754,"user_tz":-120,"elapsed":197,"user":{"displayName":"Philipp RÃ¶ssler","photoUrl":"","userId":"08136704935029710025"}},"outputId":"fd2174c4-c895-4514-a0cf-266a86c3ad95"},"source":["# free some RAM if training was conducted before\n","x_train,y_train,x_val,y_val = 0,0,0,0\n","# create list of trained models\n","write_log('Model evaluation started ...')\n","model_names = os.listdir(paths['models'])\n","# iterate models\n","for model_name in model_names:\n","  # get label and split from modelname\n","  net,label,split_index = model_name.rstrip(\".h5\").split(\"/\")[-1].split(\"_\")\n","  # set saving paths for the results\n","  evaluation_path = os.path.join(paths['evaluation'],'evaluation_'+label+'_'+str(split_index)+'.csv')\n","  confusion_matrix_path = os.path.join(paths['evaluation'],'confusionMatrix_'+label+'_'+str(split_index)+'.csv')\n","  # check if model was already evaluated\n","  if os.path.isfile(evaluation_path) and os.path.isfile(confusion_matrix_path):\n","    write_log('Model for '+label+' '+str(split_index)+' already evaluated')\n","  else:\n","    write_log('Current model: '+label+' '+str(split_index))\n","    # load current model\n","    cur_model = load_model(os.path.join(paths['models'],model_name))\n","    # read according test data\n","    x_test, y_test = read_data(label,split_index,'test')\n","    # Reshape data, must be (no_spectrograms, 227,227,1) instead of (no_spectrograms, 227,227)\n","    x_test = x_test.reshape((len(x_test), img_width, img_height, img_num_channels))\n","    # Run the Class Predictions and get the probabilities\n","    start_time = time.time()\n","    pred_probas = cur_model.predict(x_test, verbose=1)\n","    predictions = np.argmax(pred_probas,axis=1)\n","    # calculate evaluation metrics\n","    if os.path.isfile(evaluation_path) == False:\n","      evaluation = pd.DataFrame(data=np.zeros((1,7),dtype=np.float), columns=['label','split','accuracy','balanced accuracy','precision','recall','kappa'])\n","      evaluation['label'] = label\n","      evaluation['split'] = split_index\n","      evaluation['accuracy'] = accuracy_score(y_test, predictions)\n","      evaluation['balanced accuracy'] = balanced_accuracy_score(y_test, predictions)\n","      evaluation['precision'] = precision_score(y_test, predictions, average='weighted')\n","      evaluation['recall'] = recall_score(y_test, predictions, average='weighted')\n","      evaluation['kappa'] = cohen_kappa_score(y_test, predictions)\n","      #roc_AUC = roc_auc_score(y_test, pred_probas) # not working for multiclass?!\n","      # calculate accuracy and test loss\n","      # score = cur_model.evaluate(x_test, y_test, verbose=0)\n","      # test_loss = score[0]\n","      # accuracy = score[1]\n","      evaluation.to_csv(evaluation_path, index=False)\n","    # calculate confusion matrix\n","    if os.path.isfile(confusion_matrix_path) == False:\n","      confusion_matrix = pd.crosstab(y_test, predictions, rownames=['Actual'], colnames=['Predicted'])\n","      confusion_matrix.to_csv(confusion_matrix_path, index= False)\n","    write_log('Evaluation of model '+label+' '+str(split_index)+' finished in '+str(get_duration(start_time,time.time())))\n","write_log('All evaluations finished!')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model evaluation started ...\n","Model for gender 0 already evaluated\n","Model for gender 1 already evaluated\n","Model for gender 2 already evaluated\n","Model for gender 3 already evaluated\n","Model for digit 0 already evaluated\n","Model for digit 1 already evaluated\n","Model for digit 2 already evaluated\n","Model for digit 3 already evaluated\n","Model for digit 4 already evaluated\n","All evaluations finished!\n"]}]},{"cell_type":"markdown","metadata":{"id":"X0N3zAsSYTjC"},"source":["## Calculate mean results"]},{"cell_type":"code","metadata":{"id":"ToTWSHLs_lsa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630662406849,"user_tz":-120,"elapsed":6654,"user":{"displayName":"Philipp RÃ¶ssler","photoUrl":"","userId":"08136704935029710025"}},"outputId":"9b9839fa-df44-48dc-e2f2-5556d7a26413"},"source":["write_log('Calculating mean results ...')\n","start_time = time.time()\n","# create variables to store the results\n","gender_evaluation_data = pd.DataFrame(data=np.zeros((0,7),dtype=np.float), columns=['label','split','accuracy','balanced accuracy','precision','recall','kappa'])\n","gender_confusion_matrix_data = pd.DataFrame(data=np.zeros((0,0),dtype=np.float))\n","digit_evaluation_data = pd.DataFrame(data=np.zeros((0,7),dtype=np.float), columns=['label','split','accuracy','balanced accuracy','precision','recall','kappa'])\n","digit_confusion_matrix_data = pd.DataFrame(data=np.zeros((0,0),dtype=np.float))\n","# define counters\n","gender_confusion_matrix_counter,digit_confusion_matrix_counter = 0,0\n","# iterate all available result files\n","for cur_evaluation_name in os.listdir(paths['evaluation']):\n","  # exclude possibly existing mean files\n","  if 'mean' in cur_evaluation_name:\n","    continue\n","  # get evaluation type, label and split from saved result csv\n","  evaluation_type,label,split_index = cur_evaluation_name.rstrip(\".csv\").split(\"/\")[-1].split(\"_\")\n","  # read results of the models\n","  cur_data = pd.read_csv(os.path.join(paths['evaluation'],cur_evaluation_name))\n","  # add current results to the correct data collection\n","  if evaluation_type == 'evaluation':\n","    if label == 'gender':\n","      gender_evaluation_data = pd.concat((gender_evaluation_data,cur_data),axis=0,sort=False)\n","    else:\n","      digit_evaluation_data = pd.concat((digit_evaluation_data,cur_data),axis=0,sort=False)\n","  else:\n","    if label == 'gender':\n","      gender_confusion_matrix_data = gender_confusion_matrix_data.add(cur_data, fill_value=0)\n","      gender_confusion_matrix_counter += 1\n","    else:\n","      digit_confusion_matrix_data = digit_confusion_matrix_data.add(cur_data, fill_value=0)\n","      digit_confusion_matrix_counter += 1\n","# calculate mean values\n","gender_evaluation_data.loc['mean'] = gender_evaluation_data.mean()\n","gender_evaluation_data.at['mean', 'label'] ='gender'\n","gender_evaluation_data.at['mean', 'split'] ='mean'\n","digit_evaluation_data.loc['mean'] = digit_evaluation_data.mean()\n","digit_evaluation_data.at['mean', 'label'] ='digit'\n","digit_evaluation_data.at['mean', 'split'] ='mean'\n","gender_confusion_matrix_mean = gender_confusion_matrix_data/gender_confusion_matrix_counter\n","digit_confusion_matrix_mean = digit_confusion_matrix_data/digit_confusion_matrix_counter\n","# save mean results to csv\n","gender_evaluation_data.to_csv(os.path.join(paths['evaluation'],'evaluation_gender_mean.csv'),index = False)\n","digit_evaluation_data.to_csv(os.path.join(paths['evaluation'],'evaluation_digit_mean.csv'),index = False)\n","gender_confusion_matrix_mean.to_csv(os.path.join(paths['evaluation'],'confusionMatrix_gender_mean.csv'),index = False)\n","digit_confusion_matrix_mean.to_csv(os.path.join(paths['evaluation'],'confusionMatrix_digit_mean.csv'),index = False)\n","write_log('Mean results calculated in '+get_duration(start_time,time.time()))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Calculating mean results ...\n","Mean results calculated in 00:00:06\n"]}]},{"cell_type":"code","metadata":{"id":"TZWu2qbzHczO"},"source":["# # not working for multiclass\n","# fprT, tprT, _ = roc_curve(y_test, predictions)\n","\n","# plt.figure(figsize=(9,7))\n","\n","# plt.plot(fprT, tprT, label='ROC Test Split (AUC: %.2f)' %  roc_auc_score(y_test, predictions))\n","# plt.plot([0, 1], [0, 1], linestyle='--', color='k')\n","# plt.xlim([-0.03, 1.03])\n","# plt.ylim([-0.03, 1.03])\n","# plt.xlabel('False Positive Rate')\n","# plt.ylabel('True Positive Rate')\n","# plt.title('ROC Curve: SimpleCNN Cats vs Dogs CNN')\n","# plt.legend(loc=\"lower right\", facecolor='white', edgecolor='white')\n","# plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PTKPBDtjHpu1"},"source":["# history_saving_path = os.path.join(paths['history'],'AlexNet_gender_0.pkl')\n","# history = pickle.load(open(history_saving_path,'rb'))\n","\n","# plt.figure(figsize=(9,7))\n","# plt.plot(history['loss'])\n","# plt.plot(history['val_loss'])\n","# plt.title('Train loss')\n","# plt.ylabel('Model Loss')\n","# plt.xlabel('Epoch')\n","# plt.legend(['train', 'validate'], loc='center right')\n","# plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NOs_BcY-O4rs"},"source":["# 5. Run XAI methods"]},{"cell_type":"markdown","metadata":{"id":"TcOe6rWx0Zg4"},"source":["## Gradient-weighted Class Activation Mapping (Grad-CAM)"]},{"cell_type":"code","metadata":{"id":"e2vFfZxoO46T"},"source":["xai_methods = ['Grad-CAM','LIME','LRP','DTD','OCC']\n","# create list of trained models\n","write_log('XAI methods started ...')\n","model_names = os.listdir(paths['models'])\n","# iterate models\n","for model_name in model_names:\n","  # load current model\n","  cur_model = load_model(os.path.join(paths['models'],model_name))\n","  # get label and split from modelname\n","  net,label,split_index = model_name.rstrip(\".h5\").split(\"/\")[-1].split(\"_\")\n","\n","\n","\n","\n","#   # set saving paths for the results\n","#   evaluation_path = os.path.join(paths['evaluation'],'evaluation_'+label+'_'+str(split_index)+'.csv')\n","#   confusion_matrix_path = os.path.join(paths['evaluation'],'confusionMatrix_'+label+'_'+str(split_index)+'.csv')\n","#   # check if model was already evaluated\n","#   if os.path.isfile(evaluation_path) and os.path.isfile(confusion_matrix_path):\n","#     write_log('Model for '+label+' '+str(split_index)+' already evaluated')\n","#   else:\n","#     write_log('Current model: '+label+' '+str(split_index))\n","#     # read according test data\n","#     x_test, y_test = read_data(label,split_index,'test')\n","#     # Reshape data, must be (no_spectrograms, 227,227,1) instead of (no_spectrograms, 227,227)\n","#     x_test = x_test.reshape((len(x_test), img_width, img_height, img_num_channels))\n","#     # Run the Class Predictions and get the probabilities\n","#     start_time = time.time()\n","#     pred_probas = cur_model.predict(x_test, verbose=1)\n","#     predictions = np.argmax(pred_probas,axis=1)\n","#     # calculate evaluation metrics\n","#     if os.path.isfile(evaluation_path) == False:\n","#       evaluation = pd.DataFrame(data=np.zeros((1,7),dtype=np.float), columns=['label','split','accuracy','balanced accuracy','precision','recall','kappa'])\n","#       evaluation['label'] = label\n","#       evaluation['split'] = split_index\n","#       evaluation['accuracy'] = accuracy_score(y_test, predictions)\n","#       evaluation['balanced accuracy'] = balanced_accuracy_score(y_test, predictions)\n","#       evaluation['precision'] = precision_score(y_test, predictions, average='weighted')\n","#       evaluation['recall'] = recall_score(y_test, predictions, average='weighted')\n","#       evaluation['kappa'] = cohen_kappa_score(y_test, predictions)\n","#       #roc_AUC = roc_auc_score(y_test, pred_probas) # not working for multiclass?!\n","#       # calculate accuracy and test loss\n","#       # score = cur_model.evaluate(x_test, y_test, verbose=0)\n","#       # test_loss = score[0]\n","#       # accuracy = score[1]\n","#       evaluation.to_csv(evaluation_path, index=False)\n","#     # calculate confusion matrix\n","#     if os.path.isfile(confusion_matrix_path) == False:\n","#       confusion_matrix = pd.crosstab(y_test, predictions, rownames=['Actual'], colnames=['Predicted'])\n","#       confusion_matrix.to_csv(confusion_matrix_path, index= False)\n","#     write_log('Evaluation of model '+label+' '+str(split_index)+' finished in '+str(get_duration(start_time,time.time())))\n","# write_log('All evaluations finished!')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q2cWooc_5opr"},"source":["from gradcam import VizGradCAM"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":129},"id":"F57dbMaXANmh","executionInfo":{"status":"error","timestamp":1630511076660,"user_tz":-120,"elapsed":227,"user":{"displayName":"Philipp RÃ¶ssler","photoUrl":"","userId":"08136704935029710025"}},"outputId":"84c1e646-44a8-42d7-d416-bd8f434173af"},"source":["#test_img = img_to_array(load_img(\"monkey.jpeg\" , target_size=(224,224)))\n","test_model = load_model(os.path.join(paths['models'],'AlexNet_gender_0.h5'))\n","test_img = /gdrive/My Drive/Xplainable AI - Univariate Time Series - spectrograms/spectrograms/38/AlexNet_0_38_0.hdf5\n","\n","# Use The Function - Boom!\n","VizGradCAM(test_model, test_img))"],"execution_count":null,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"ignored","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-16-031babfd46db>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    test_img = /gdrive/My Drive/Xplainable AI - Univariate Time Series - spectrograms/spectrograms/38/AlexNet_0_38_0.hdf5\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}]},{"cell_type":"code","metadata":{"id":"QakrbNSUBqvG"},"source":["#test_model = load_model(os.path.join(paths['models'],'AlexNet_gender_0.h5'))\n","cur_path = '/gdrive/My Drive/Xplainable AI - Univariate Time Series - spectrograms/spectrograms/38/AlexNet_0_38_0.hdf5'\n","f = h5py.File(cur_path, 'r')\n","x_cur = f['data'][...]\n","y_cur = f['label'][...]\n","f.close()\n","\n","x_cur = x_cur[0][0]\n","y_cur = y_cur[0][0] \n","\n","# Reshape data, must be (no_spectrograms, 227,227,1) instead of (no_spectrograms, 227,227)\n","x_cur = x_cur.reshape(1, 227,227,1)\n","\n","x_cur.shape\n","VizGradCAM(test_model, x_cur)\n","\n","# index = 0\n","#   x = np.zeros(((len(split_paths),227,227))) # create target array for spectrograms\n","#   y = np.zeros(len(split_paths)) # create target array for labels\n","#   for cur_path in split_paths: # iterate the files\n","#     #read current file\n","#     f = h5py.File(cur_path, 'r')\n","#     x_cur = f['data'][...]\n","#     y_cur = f['label'][...]\n","#     f.close() \n","#     #extract relevant data of current file\n","#     x_cur = x_cur[0][0]\n","#     y_cur = y_cur[0][label_index]    \n","#     #append current data to x and y\n","#     x[index] = x_cur\n","#     y[index] = y_cur\n","#     # increase index by 1\n","#     index +=1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xbozI8EgGIcE"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v5aeybfw0e2A"},"source":["## Local interpretable model-agnostic explanations (LIME)"]},{"cell_type":"code","metadata":{"id":"rKrcGl4W0e_c"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pibi0Yi50fHA"},"source":["## Layerwise Relevance Propagation (LRP)"]},{"cell_type":"code","metadata":{"id":"LG4KN9RA0fNx"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8SSTmueo-fIT"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ddlofqNu0fUw"},"source":["## Deep Taylor Decomposition (DTD)"]},{"cell_type":"code","metadata":{"id":"jXHGVmMt0faR"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"coe-A9bf0fhB"},"source":["## Occlusion Analysis (OCC)"]},{"cell_type":"code","metadata":{"id":"0empQzNU0fmo"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0D6d02-9nunk"},"source":["# Additional inspections"]},{"cell_type":"markdown","metadata":{"id":"0VkZLLEkn1Jf"},"source":["## Create plots of waveform/spectrogram data"]},{"cell_type":"markdown","metadata":{"id":"fLfNh2RMuHbO"},"source":["### functions for single files"]},{"cell_type":"code","metadata":{"id":"WBGELoM3n0qW"},"source":["# function creates a waveform plot for a single .wav file\n","# src is the path to the .wav file\n","# dst is the path were the plot is saved to\n","# optional the y-axis can be scaled between -1 and 1 (default True)\n","# optional the plot can be shown (default False)\n","def create_waveform_plot(src,dst,scale=True,show=False):\n","  if os.path.isfile(dst):\n","    return\n","  create_directory(os.path.join(paths['plots_waveform'],dst.split('/')[-2])) #create the subfolder with participant number, eg. 01\n","  fs, data = wavf.read(src)\n","  if scale:\n","    data = minmax_scale(data,feature_range=(-1,1),axis=0,copy=True) # optional: scales the y-axis between -1 and 1\n","  duration = len(data)/fs #duration of the file\n","  time = np.arange(0,duration,1/fs) #time vector\n","  # sometimes time and data dont have the same length which causes an error: fix by adjusting the length\n","  if len(time) != len(data):\n","    if len(time)>len(data):\n","      time = time[0:len(data)]\n","    else:\n","      data = data[0:len(time)]\n","  plt.plot(time,data)\n","  plt.xlabel('Time [s]')\n","  plt.ylabel('Amplitude')\n","  plt.savefig(dst)\n","  if show:\n","    plt.show()\n","  plt.close()\n","  return\n","\n","# function creates a spectrogram plot for a single .hdf5 file - analog to the function for waveform files\n","def create_spectrogram_plot(src,dst,show=False):\n","  if os.path.isfile(dst):\n","    return\n","  create_directory(os.path.join(paths['plots_spectrograms'],dst.split('/')[-2])) #create the subfolder with participant number, eg. 01\n","  #read current file\n","  f = h5py.File(src, 'r')\n","  data = f['data'][...]\n","  f.close() \n","  data = np.squeeze(data) # alternatively use: data = data[0,0]\n","  plt.pcolormesh(data, cmap='Greys')\n","  plt.axis('off')\n","  #plt.xlabel('Time [s]')\n","  #plt.ylabel('Hz')\n","  plt.savefig(dst)\n","  if show:\n","    plt.show()\n","  plt.close()\n","  return"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b6xEiZiJuLV9"},"source":["### functions to iterate all files or some examples"]},{"cell_type":"code","metadata":{"id":"kzeHlp4yDJRx"},"source":["# if you want to create the waveform plots for all .wav files or some examples for each participant, the following function iterates the files\n","def create_plots(src, dst, src_meta, input_type, n_processes=5, mode = 'examples', nb_examples = 3,scale=True):\n","  if mode == 'examples':\n","    write_log('Started creating plots of '+input_type+' for '+str(nb_examples)+' examples for each digit of each participant ...')\n","  elif mode == 'all':\n","    write_log('Started creating plots of '+input_type+' for all files ...')\n","  else:\n","    write_log('No valid mode!')\n","  start_time = time.time()\n","  folders = []\n","  for folder in os.listdir(src):\n","    # only process folders\n","    if not os.path.isdir(os.path.join(src, folder)):\n","      continue\n","    folders.append(folder)\n","  # start multiprocessing to speed up the creation of the png files\n","  pool=multiprocessing.Pool(processes=n_processes)\n","  _=pool.map(_create_plots, [(os.path.join(src, folder), \n","                                          os.path.join(dst, folder), \n","                                          src_meta,input_type,mode,nb_examples,scale) for folder in sorted(folders)])\n","  write_log('Finished creating plots in '+str(get_duration(start_time,time.time())))\n","\n","def _create_plots(parameters):\n","  src, dst, src_meta, input_type, mode, nb_examples,scale = parameters\n","  #print(\"processing {}\".format(src))\n","  metaData = json.load(open(src_meta))\n","  # create folder for png files\n","  if not os.path.exists(dst):\n","    os.makedirs(dst)\n","  # check input type and get corresponding file format\n","  if input_type == 'waveform':\n","    file_format = '.wav'\n","  elif input_type == 'spectrogram':\n","    file_format = '.hdf5'\n","  else:\n","    write_log('No valid input type!')\n","    return\n","  # define counter\n","  counter = [0,0,0,0,0,0,0,0,0,0]   \n","  # loop over recordings\n","  for filepath in sorted(glob.glob(os.path.join(src, '*'+file_format))):\n","    # distinguish between waveform and spectrograms   \n","    if input_type == 'waveform':     \n","      dig, vp, rep = filepath.rstrip(file_format).split(\"/\")[-1].split(\"_\") # infer sample info from name\n","      if counter[int(dig)] == int(nb_examples): # continue if enough examples are created for the current digit\n","        continue\n","      gender = 0 if metaData[vp][\"gender\"] == \"male\" else 1 # get gender\n","      saving_path = os.path.join(dst,str(gender)+'_'+str(dig)+'_'+str(vp)+'_'+str(rep)+'.png') # define saving path\n","      create_waveform_plot(filepath,saving_path, show=False, scale=scale)\n","    else:\n","      net, dig, vp, rep = filepath.rstrip(\".hdf5\").split(\"/\")[-1].split(\"_\") # infer sample info from name\n","      if counter[int(dig)] == int(nb_examples): # continue if enough examples are created for the current digit\n","        continue\n","      gender = 0 if metaData[vp][\"gender\"] == \"male\" else 1 # get gender\n","      saving_path = os.path.join(dst,net+'_'+str(gender)+'_'+str(dig)+'_'+str(vp)+'_'+str(rep)+'.png')# define saving path\n","      create_spectrogram_plot(filepath,saving_path,show=False) \n","    # check if all files should be iterated, if not: increase counter by 1\n","    if mode == 'all':\n","      continue\n","    else:\n","      counter[int(dig)] +=1     \n","  return "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9BL8CyBD8kDZ"},"source":["### Select and start functions"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ByLlEtkQ7MHc","executionInfo":{"status":"ok","timestamp":1630623172761,"user_tz":-120,"elapsed":52523,"user":{"displayName":"Philipp RÃ¶ssler","photoUrl":"","userId":"08136704935029710025"}},"outputId":"7355c785-3add-48d8-a563-0f2e90d47e32"},"source":["# create one waveform plot\n","#create_waveform_plot(os.path.join(paths['data'],'56/5_56_0.wav'),os.path.join(paths['plots_waveform'],'56/1_5_56_0.png'),show=True)\n","# create all or example plots for wavelet files\n","create_plots(src=paths['data'],dst=paths['plots_waveform'],src_meta=paths['meta'],mode='examples',input_type='waveform',nb_examples=3,scale=True, n_processes=5)\n","# create one spectrogram plot\n","#create_spectrogram_plot(os.path.join(paths['spectrograms'],'56/AlexNet_5_56_0.hdf5'),os.path.join(paths['plots_spectrograms'],'56/AlexNet_1_5_56_0.png'),show=True)\n","# create all or example plots for spectrogram files\n","create_plots(src=paths['spectrograms'],dst=paths['plots_spectrograms'],src_meta=paths['meta'],mode='examples',input_type='spectrogram',nb_examples=3, n_processes=5)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Started creating plots of spectrogram for 3 examples for each digit of each participant ...\n","Finished creating plots in 00:00:52\n"]}]},{"cell_type":"markdown","metadata":{"id":"o_myILtXn8BX"},"source":[""]},{"cell_type":"code","metadata":{"id":"-2zTz68HAfdI"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"l8j6oQu6IXR6"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CFRmjxOsJRQO"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9f3WCNkvOPh3"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B3SrivdHM1JW"},"source":[""]}]}